{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d75a0c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009114,
     "end_time": "2024-04-21T00:21:41.792956",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.783842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "CNNs are a special type of Neural Networks for prediction over data that has a grid-like topology.\n",
    "\n",
    "## Input\n",
    "The input to a CNN is a volume of width $W_1$, height $H_1$, and depth $D_1$. While the width and the height refer to the resolution of the input, e.g. the number of pixels if the input is an image, the depth refers to the number of features for each grid. In the case of an image, the depth is tipically $3$ containing RGB data.\n",
    "\n",
    "### Example\n",
    "The input volume below has width $W=10$, height $H=10$, and depth $D=3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223bd4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:41.812971Z",
     "iopub.status.busy": "2024-04-21T00:21:41.812305Z",
     "iopub.status.idle": "2024-04-21T00:21:41.824996Z",
     "shell.execute_reply": "2024-04-21T00:21:41.824074Z"
    },
    "papermill": {
     "duration": 0.025922,
     "end_time": "2024-04-21T00:21:41.827622",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.801700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "W, H, D = 10, 10, 3\n",
    "input_volume = np.ones([W, H, D])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445fe106",
   "metadata": {
    "papermill": {
     "duration": 0.00841,
     "end_time": "2024-04-21T00:21:41.844560",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.836150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Kernel\n",
    "Convolutions are applied over an input volume and a kernel, $w$. In Convolutional Neural Networks, the width and height of the kernels are tipically square, i.e. they have the same width and height. The value for the width and height of a kernel is given by the receptive field, $F$. The receptive field indicates the amount of elements in width and height that the kernel is covering by placing it over the volume to be convolved width. The depth of each kernel is **always** equal to the depth of the input volume.\n",
    "\n",
    "## Example\n",
    "In the example below, the kernel `kernel` has a receptive field $F=2$ and a depth equal to the depth of `input_volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c9fe4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:41.864641Z",
     "iopub.status.busy": "2024-04-21T00:21:41.863889Z",
     "iopub.status.idle": "2024-04-21T00:21:41.869964Z",
     "shell.execute_reply": "2024-04-21T00:21:41.868772Z"
    },
    "papermill": {
     "duration": 0.019274,
     "end_time": "2024-04-21T00:21:41.872592",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.853318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The kernel has widht 2, height 2, and depth 3\n",
    "F = 2\n",
    "kernel = np.ones([F, F, D])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828c05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T23:42:52.264496Z",
     "iopub.status.busy": "2024-04-18T23:42:52.264079Z",
     "iopub.status.idle": "2024-04-18T23:42:52.274101Z",
     "shell.execute_reply": "2024-04-18T23:42:52.272528Z",
     "shell.execute_reply.started": "2024-04-18T23:42:52.264446Z"
    },
    "papermill": {
     "duration": 0.008425,
     "end_time": "2024-04-21T00:21:41.889794",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.881369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convolution\n",
    "The fundamental operation of a convolutional layer is the convolution. Given an input and a kernel, the convolution produces a scalar value by applying the kernel to the input.\n",
    "\n",
    "Althought there are different types of convolutional operations, the most common is computed by summing up the matrix elements from the element-wise multiplication of the input and the kernel.\n",
    "\n",
    "## Example\n",
    "Given two 2D matrices of equal size, the convolution can be computed as shown below using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02343709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:41.909198Z",
     "iopub.status.busy": "2024-04-21T00:21:41.908792Z",
     "iopub.status.idle": "2024-04-21T00:21:41.917038Z",
     "shell.execute_reply": "2024-04-21T00:21:41.915857Z"
    },
    "papermill": {
     "duration": 0.0207,
     "end_time": "2024-04-21T00:21:41.919273",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.898573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([[1, 2], [2, 4]])\n",
    "kernel = np.array([[1, 1], [2, 2]])\n",
    "C = np.sum(x * kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e574c",
   "metadata": {
    "papermill": {
     "duration": 0.008159,
     "end_time": "2024-04-21T00:21:41.936030",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.927871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tensors\n",
    "\n",
    "In Deep Learning, the input is a volume with width, $W$, height, $H$, and depth, $D$. While the width and height represent the grid-like topology, the depth dimension represents data over the grid. In the case of images, the depth contains each of the color channels, R, G, and B, of input images.\n",
    "\n",
    "Because of the multidimensional nature of input volumes, Deep Learning libraries use tensors to represent them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Below, `x` is extended to an input volume with $D=3$. The depth of the kernel `kernel` should match the depth of the volume `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e67d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:41.955581Z",
     "iopub.status.busy": "2024-04-21T00:21:41.954390Z",
     "iopub.status.idle": "2024-04-21T00:21:41.961930Z",
     "shell.execute_reply": "2024-04-21T00:21:41.961028Z"
    },
    "papermill": {
     "duration": 0.019804,
     "end_time": "2024-04-21T00:21:41.964381",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.944577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([ [[1, 2], [2, 4]], [[1, 2], [2, 4]], [[1, 2], [2, 4]] ])\n",
    "kernel = np.array([ [[1, 1], [2, 2]], [[1, 1], [2, 2]], [[1, 1], [2, 2]] ])\n",
    "C = np.sum(x * kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfe920",
   "metadata": {
    "papermill": {
     "duration": 0.009571,
     "end_time": "2024-04-21T00:21:41.982664",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.973093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The same operation can be performed with `numpy.einsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81a6959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.001863Z",
     "iopub.status.busy": "2024-04-21T00:21:42.001423Z",
     "iopub.status.idle": "2024-04-21T00:21:42.006728Z",
     "shell.execute_reply": "2024-04-21T00:21:42.005409Z"
    },
    "papermill": {
     "duration": 0.017893,
     "end_time": "2024-04-21T00:21:42.009159",
     "exception": false,
     "start_time": "2024-04-21T00:21:41.991266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "C2 = np.einsum('ijk,ijk->', x, kernel)\n",
    "assert C == C2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c52e76",
   "metadata": {
    "papermill": {
     "duration": 0.008192,
     "end_time": "2024-04-21T00:21:42.026103",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.017911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the example above, the first parameter to the `numpy.einsum` function encodes the sum and element-wise multiplication. The first _ijk_ term refers to the dimensions of the tensor `x`. _i_ is the depth, _j_ is the height, and _k_ is the width. The second _ijk_ term refers to the width, height, and depth of the tensor `kernel`. By using the same characters for both tensors, we indicate `numpy.einsum` to perform an element-wise multiplication among them. By not adding any character after _->_, we indicate `numpy.einsum` to sum all the elements that result from the element-wise multiplication and return a scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a421d23",
   "metadata": {
    "papermill": {
     "duration": 0.008257,
     "end_time": "2024-04-21T00:21:42.042970",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.034713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simplification\n",
    "\n",
    "From now on, for simplification we regard the width, $W$, of the input volume to be the same as its height, $H$, and refer to both of them as width, $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd0ec3",
   "metadata": {
    "papermill": {
     "duration": 0.008236,
     "end_time": "2024-04-21T00:21:42.060341",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.052105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolutional layer\n",
    "\n",
    "The output of a convolutional layer is not a scalar value but an output volume produced by sliding and convolving kernels over the input. The width of the output volume that is produced from convolving a kernel with receptive field, $F$, with the input volume with width, $W$, is $W - F + 1$.\n",
    "\n",
    "In Convolutional Neural Networks, while the input has large values for $W$ and $H$, the receptive field, $F$, of the kernels is comparatively small in size.\n",
    "\n",
    "### Example I\n",
    "\n",
    "Provided the input volume, `x`, and the kernel, `kernel`, the output of the convolutional layer can be computed using NumPy as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e83381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.079474Z",
     "iopub.status.busy": "2024-04-21T00:21:42.079053Z",
     "iopub.status.idle": "2024-04-21T00:21:42.086852Z",
     "shell.execute_reply": "2024-04-21T00:21:42.085739Z"
    },
    "papermill": {
     "duration": 0.020103,
     "end_time": "2024-04-21T00:21:42.089045",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.068942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W, D, F = 10, 2, 2\n",
    "x = np.random.rand(D, W, W)\n",
    "kernel = np.random.rand(D, F, F)\n",
    "\n",
    "W_out = W - F  + 1\n",
    "x_out = np.ones((1, W_out, W_out))\n",
    "for row in range(W_out):\n",
    "    for col in range(W_out):\n",
    "        x_out[0, row, col] = np.einsum('ijk,ijk->', x[:,row:row+F,col:col+F], kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041d7a9",
   "metadata": {
    "papermill": {
     "duration": 0.008165,
     "end_time": "2024-04-21T00:21:42.105927",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.097762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Example II\n",
    "A convolutional layer convolves the volume over a collection of $K$ filters, where each filter has the same receptive field, $F$. The depth of each filter is the same depth, $D$, as the input volume. The example below is a demonstration of a convolutional layer with $K=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c663aecc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.125336Z",
     "iopub.status.busy": "2024-04-21T00:21:42.124244Z",
     "iopub.status.idle": "2024-04-21T00:21:42.133036Z",
     "shell.execute_reply": "2024-04-21T00:21:42.131845Z"
    },
    "papermill": {
     "duration": 0.021255,
     "end_time": "2024-04-21T00:21:42.135726",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.114471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W, D, F, K = 10, 2, 2, 2\n",
    "x = np.random.rand(D, W, W)\n",
    "kernels = np.random.rand(K, D, F, F)\n",
    "\n",
    "W_out = W - F  + 1\n",
    "x_out = np.ones((K, W_out, W_out))\n",
    "for row in range(W_out):\n",
    "    for col in range(W_out):\n",
    "        x_out[:, row, col] = np.einsum('ijk,hijk->h', x[:,row:row+F,col:col+F], kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ed4cd",
   "metadata": {
    "papermill": {
     "duration": 0.008245,
     "end_time": "2024-04-21T00:21:42.152652",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.144407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Zero-padding\n",
    "\n",
    "Sliding a kernel over an input volume to produce an output has the effects listed below.\n",
    "- Stacking Convolutional Networks leads to volumes that are smaller and smaller in size.\n",
    "- The closer an element from the input volume is to its borders, the less amount of elements it influences on the output volume.\n",
    "\n",
    "A strategy to gain control over these phenomena is to enlarge the input volume by adding zeros to its borders. This technique is called zero-padding. Adding zero-padding before convolution can result into an output volume that preserves the original size of the input volume.\n",
    "\n",
    "The amount of applied zero-padding is represented with $P$. Adding zero-padding $P$ increases the width, $W$, of the input volume by $2P$. Sliding the kernel over the zero-padded input volume results into an output volume of width $W + 2P -F + 1$.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Given the input volume, `x`, with values $W=10$ and $D=1$, and a filter $F=5$. Zero-padding the input volume with $P=2$, results into an output volume of the same size as the input volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34ae18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.171799Z",
     "iopub.status.busy": "2024-04-21T00:21:42.171336Z",
     "iopub.status.idle": "2024-04-21T00:21:42.182519Z",
     "shell.execute_reply": "2024-04-21T00:21:42.181252Z"
    },
    "papermill": {
     "duration": 0.023789,
     "end_time": "2024-04-21T00:21:42.184908",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.161119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W, D, F = 10, 1, 5\n",
    "x = np.random.rand(D, W, W)\n",
    "kernel = np.random.rand(D, F, F)\n",
    "\n",
    "P = 2\n",
    "W_out = W - F + 2*P + 1\n",
    "\n",
    "x_out = np.zeros((kernel.shape[0], W_out, W_out))\n",
    "assert x_out.shape == x.shape\n",
    "x = np.pad(x,((0,0),(P,P),(P,P)))\n",
    "\n",
    "for row in range(W_out):\n",
    "    for col in range(W_out):\n",
    "        x_out[:, row, col] = np.einsum('ijk,ijk->',x[:,row:row+F,col:col+F], kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be3fe8",
   "metadata": {
    "papermill": {
     "duration": 0.008147,
     "end_time": "2024-04-21T00:21:42.201679",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.193532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stride\n",
    "Previous convolutional layer demonstrations apply the kernel over all valid locations in the input volume, i.e. over all locations in the input volume where the kernel properly fits without non-overlapping areas between the kernel and the input volume. \n",
    "\n",
    "It is possible, however, to systematically apply the kernel over a subset of all valid locations by skipping a determined number of cells when sliding it. This is known as the stride $S$, which defines the amount of cells that are skipped per slide.\n",
    "\n",
    "Given the input volume with width, $W$, and the kernel width receptive field, $F$, convolving the input volume with the kernel with zero-padding, $P$, and stride, $S$, results into an output volume of width $\\frac{W - F + 2P}{S}+1$.\n",
    "\n",
    "Selecting the values for the receptive field, $F$, zero-padding, $P$, and stride, $S$ is tricky, as not all combinations properly fit over the input volume. Only combinations for which $\\frac{W - F + 2P}{S}+1$ results into a natural number are valid.\n",
    "\n",
    "Striding is a technique to downsample the input volume. The larger the $S$ value, the more aggresive the downsampling is. Commonly, $S=2$ or $S=3$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Given an input volume with width, $W$, the following values for the stride, $S$, receptive field, $F$, and zero-padding, $P$, are a _valid_ combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfef688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.220637Z",
     "iopub.status.busy": "2024-04-21T00:21:42.219830Z",
     "iopub.status.idle": "2024-04-21T00:21:42.229245Z",
     "shell.execute_reply": "2024-04-21T00:21:42.228373Z"
    },
    "papermill": {
     "duration": 0.021634,
     "end_time": "2024-04-21T00:21:42.231728",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.210094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, W, F = 3, 10, 5\n",
    "x = np.random.rand(D, W, W)\n",
    "kernel = np.random.rand(D, F, F)\n",
    "S = 2\n",
    "P = 2\n",
    "\n",
    "W_out = int((W-F+2*P) / S) + 1\n",
    "x = np.pad(x,((0,0),(P,P),(P,P)))\n",
    "x_out = np.zeros((x.shape[0], W_out, W_out))\n",
    "\n",
    "for row_out, row_in in zip(range(0, W_out), range(0, W, 2)):\n",
    "    for col_out, col_in in zip(range(0, W_out), range(0, W, 2)):\n",
    "        x_out[:,row_out, col_out] = np.einsum('ijk,ijk->', x[:,row_in:row_in+F,col_in:col_in+F], kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f970abc",
   "metadata": {
    "papermill": {
     "duration": 0.008093,
     "end_time": "2024-04-21T00:21:42.248385",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.240292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Kernels as shared parameters\n",
    "So far convolutions and kernels have been introduced without explaining their meaning, purpose, and origin.\n",
    "\n",
    "### Origin\n",
    "The origin of convolutions can be backtracked to filtering techniques for image processing. Given an input image $x$, filters modify the original image to extract feature information thas is valuable for further image processing stages. A common approach for image filtering is to use a neighboring window of adjacent cells and feed it to a function that produces a scalar value for each pixel. If the size of the window is given by the receptive field, $F$, and the selected pixels from that window are passed through a function that is a convolution, then we have something that resembles the operation previously described in the context of Deep Learning.\n",
    "\n",
    "### Purpose\n",
    "The objective of convolving an input volume over a kernel using a sliding window is to extract information, features, that could be relevant for further processing stages. While traditional image processing techniques, kernels would be hand engineered, the intention of Convolutional Neural Networks is to learn those kernels. The values of a kernel are thus the weights to learn\n",
    "to during gradient descent.\n",
    "\n",
    "### Meaning of kernels as neurons\n",
    "Given an input, $x$, and a kernel $w$, where $x$ and $w$ have the same width, length, and depth. If the convolution, $z = x * w$, is given by the sum of the element-wise multiplication of $x$ and $w$, then $z$ is the weighted sum of $x$, and $w$. This operation is identical to weighted sum produced between the output from an input layer and the weights from a neuron in a hidden or output layer in Feed Forward Neural Networks. A kernel can then be regarded as a neuron, and the collection of $K$ kernels in a Convolutional Layer is the amount of neurons in that layer.\n",
    "\n",
    "### Infinite prior\n",
    "In Convolutional Neural Networks, the input volume is significantly larger than the sizes of the $K$ kernels in a Convolutional Layer. Given that kernels contain the weights to be learned, if they are slided throuh the input, each convolution between a region in the input volume and a kernel can be considered as an operation between an input layer and a neuron. If the input volume is regarded as the input layer, it is considered that the weights of the neuron for the region of the input volume that lie outside the receptive field, $F$, of the kernel are always zero.\n",
    "\n",
    "Ian Goodfellow refers to this as the infinitely strong prior.\n",
    "\n",
    "### Shared weights\n",
    "A Convolutional Layer applies the same kernel across the whole input volume with different patterns, defined by the stride $S$, the zero-padding, $P$, and the receptive field, $F$. Since each of these convolutions between a section in the input volume and a kernel is a scalar value containing a weighted sum, we can say that either the same neuron, kernel, is being applied to different input patches, or that we are using different neurons per patch that share the same weights. It is because of this that several authors mention that kernels contain shared weights to be learned during training, i.e. instead of assigning a unique weight to each _input_ (_input_ here is a section of the whole input volume), weights are shared across the _inputs_.\n",
    "\n",
    "## Bias\n",
    "\n",
    "If each kernel is regarded as a neuron, then each neuron is missing the bias parameter. By adding a bias parameter after the convolution, we can modify our interpretation of a kernel, not as a neuron, but as the weights of a neuron. Biases are shared in the same way as weights.\n",
    "\n",
    "### Example\n",
    "Applying $K$ kernels over an input volume, results into a vector which size is equal to the depth of $K$. For each element of that vector, i.e. for each of the depth values in $K$, there is a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbf8348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.267571Z",
     "iopub.status.busy": "2024-04-21T00:21:42.266505Z",
     "iopub.status.idle": "2024-04-21T00:21:42.276697Z",
     "shell.execute_reply": "2024-04-21T00:21:42.275481Z"
    },
    "papermill": {
     "duration": 0.022451,
     "end_time": "2024-04-21T00:21:42.279250",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.256799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "W, D, F, K = 10, 3, 2, 4\n",
    "x = np.random.rand(D, W, W)\n",
    "kernels = np.random.rand(K, D, F, F)\n",
    "biases = np.random.rand(K)\n",
    "\n",
    "W_out = W-F+1\n",
    "z = np.zeros((K, W_out, W_out))\n",
    "\n",
    "for row in range(W_out):\n",
    "    for col in range(W_out):\n",
    "        z[:,row, col] = np.einsum('ijk,fijk->f', x[:,row:row+F, col:col+F], kernels) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba1dc03",
   "metadata": {
    "papermill": {
     "duration": 0.008225,
     "end_time": "2024-04-21T00:21:42.296183",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.287958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Applying a non-linearity\n",
    "\n",
    "Just like the Multi Layer Perceptron, the weighted sum plus the bias is passed through a non-linear function\n",
    "\n",
    "### Example\n",
    "Below, ReLU is applied to the output volume `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b387fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.315313Z",
     "iopub.status.busy": "2024-04-21T00:21:42.314880Z",
     "iopub.status.idle": "2024-04-21T00:21:42.320518Z",
     "shell.execute_reply": "2024-04-21T00:21:42.319670Z"
    },
    "papermill": {
     "duration": 0.017971,
     "end_time": "2024-04-21T00:21:42.322718",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.304747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = z * (z>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d4e5d",
   "metadata": {
    "papermill": {
     "duration": 0.008134,
     "end_time": "2024-04-21T00:21:42.339619",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.331485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If instead of $F=5$, we use $F=4$, the combination of $W$, $F$, $P$, and $S$ becomes valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40a07e",
   "metadata": {
    "papermill": {
     "duration": 0.008064,
     "end_time": "2024-04-21T00:21:42.356096",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.348032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lessons learned\n",
    "Kernels are tipically square, i.e. they have the same width and height. The value for the width and height is given by the receptive field, $F$. **The depth of each kernel is equal to the depth of the input volume.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9d49e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T23:33:36.040442Z",
     "iopub.status.busy": "2024-04-18T23:33:36.039999Z",
     "iopub.status.idle": "2024-04-18T23:33:36.050518Z",
     "shell.execute_reply": "2024-04-18T23:33:36.048725Z",
     "shell.execute_reply.started": "2024-04-18T23:33:36.040406Z"
    },
    "papermill": {
     "duration": 0.008092,
     "end_time": "2024-04-21T00:21:42.372772",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.364680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NumPy\n",
    "Given two matrices, $A$ and $B$, `numpy.dot(A, B)`, `numpy.matmul(A, B)`, `A @ B` produce the same output by multiplying the matrices $A$ and $B$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b04d8612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T00:21:42.391525Z",
     "iopub.status.busy": "2024-04-21T00:21:42.391088Z",
     "iopub.status.idle": "2024-04-21T00:21:42.398018Z",
     "shell.execute_reply": "2024-04-21T00:21:42.396809Z"
    },
    "papermill": {
     "duration": 0.01908,
     "end_time": "2024-04-21T00:21:42.400333",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.381253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2],[3, 4]])\n",
    "B = np.array([[1, 2],[3, 8]])\n",
    "C1 = np.dot(A, B)\n",
    "C2 = np.matmul(A, B)\n",
    "C3 = A @ B\n",
    "assert np.all(C1==C2)\n",
    "assert np.all(C1==C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa61c88",
   "metadata": {
    "papermill": {
     "duration": 0.00821,
     "end_time": "2024-04-21T00:21:42.417493",
     "exception": false,
     "start_time": "2024-04-21T00:21:42.409283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`numpy.einsum` is a powerful function that can be applied to convolve volumes and kernels."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.236245,
   "end_time": "2024-04-21T00:21:42.847146",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-21T00:21:38.610901",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
